---
title: "Diagnosing Diseases Using kNN"
subtitle: "An Application of kNN to Diagnose Diabetes"
author: "Elena Boiko, Jacqueline Razo (Advisor: Dr. Cohen)"
date: "`r Sys.Date()`"
format:
  revealjs:
    auto-stretch: true
    scrollable: true
    theme: serif
    center-title-slide: true
    title-slide-attributes:
      data-background-size: contain
      data-background-position: right
    slide-number: true
    toc: false
    transition: fade
    course: Capstone Projects in Data Science
bibliography: references.bib
self-contained: false
execute:
  warning: false
  message: false
editor:
  markdown:
    wrap: 72
---


## Introduction {.center}

In healthcare, kNN has shown promise in predicting chronic diseases like
**diabetes** and **hypertension**.

In this project, we focus on how kNN can be applied and optimized to
predict **diabetes**, a critical and growing public health issue.

::: notes
kNN is a well-known algorithm that’s already been applied in medical research, including disease prediction for conditions like hypertension and diabetes.
In this project, we explored how kNN behaves with health data and how we can optimize it to improve predictive accuracy for diabetes.
:::

## Why This Matters {.center}

<span class="fragment">- Diabetes affects millions worldwide. Early
detection can improve outcomes.

<span class="fragment">- Machine learning, especially interpretable
models like **kNN**, can support diagnosis.

<span class="fragment">- Our project explores:

<span class="fragment">- How different **k values**, **distance metrics**, and
    **preprocessing techniques** affect kNN’s performance.
    
<span class="fragment">- Whether kNN is competitive with other models for this task.

::: notes
Diabetes affects over 37 million people in the U.S., and many don’t know they have it. Early detection is critical to avoid complications.
Machine learning can help — especially models like kNN that are easy to understand and implement.
We tested how different settings, such as the number of neighbors, distance metrics, and preprocessing techniques impact performance.
We also compared kNN with models like decision trees and random forests to see how it holds up.
:::

## Why We Chose kNN for This Project

<span class="fragment">Well-suited for medical datasets with small to
medium size

<span class="fragment">Easy to interpret — great for health
professionals

<span class="fragment">Flexible with minimal assumptions

<span class="fragment">Can impute missing data and detect patterns

::: notes
We chose kNN because it’s simple, interpretable, and works well on structured health data like surveys.
Unlike more complex models, kNN doesn’t make strong assumptions — it just compares similar cases.
That makes it more transparent, which is valuable in healthcare, where decision-making needs to be explainable.
:::

## Our Approach {.center}

-   Clean and preprocess real-world survey data.
-   Train kNN models with various configurations.
-   Evaluate performance and compare with tree-based models.

::: notes
Our project followed a structured approach.
We used the CDC’s diabetes health indicators dataset, which includes over 250,000 survey responses.
After cleaning and preparing the data, we trained multiple versions of kNN by changing key parameters like the number of neighbors and the distance metric.
We then compared the best kNN’s performance with decision trees and random forests to see how it performed in a real-world healthcare prediction task.
:::

## Method: kNN Overview {.center .middle}


<span class="fragment">k-Nearest Neighbors (kNN) is a **non-parametric, instance-based** learning algorithm</span>

<span class="fragment">It is a **lazy learner** — no explicit training phase is required</span>

<span class="fragment">Instead, it classifies new data based on similarity to existing labeled points</span>

---

### <span class="fragment">Classification Process:</span>

<span class="fragment">**1. Distance Calculation:**  
Measures similarity using metrics like **Euclidean** or **Manhattan** distance</span>

<span class="fragment">**2. Neighbor Selection:**  
Hyperparameter **k** defines how many nearby points to consider</span>

<span class="fragment">**3. Majority Voting:**  
The most frequent class among the **k nearest neighbors** determines the prediction</span>

::: notes
So, let’s talk about how kNN actually works
kNN is known as a lazy learner - it doesn’t train a model in advance. Instead, it stores all the data and makes predictions based on similarity.
When a new data point comes in, kNN finds the closest examples from the training set - based on distance -  and predicts the most common label among them.
This makes it intuitive and highly adaptable, which is why it’s useful in clinical applications where transparency matters.

The process starts by calculating the distance - we used both Euclidean and Manhattan distances in our tests.
Then, the model looks at the closest k data points — and this k is something we tune to get better results.
Finally, it uses majority voting: whichever class appears most often among the neighbors becomes the prediction.
:::

## Distance Calculation: {.smaller}

kNN identifies the nearest neighbors by calculating distances between points.

<div class="fragment">

**Euclidean distance:**  
$$
d = \sqrt{(X_2 - X_1)^2 + (Y_2 - Y_1)^2}
$$

</div>

<div class="fragment">

**Manhattan distance:**  
$$
d = |X_2 - X_1| + |Y_2 - Y_1|
$$

</div>

::: {style="text-align: center; margin-top: 1em;"}
```{r}
library(ggplot2)

# Add points
X1 <- 10; Y1 <- 12
X2 <- 14; Y2 <- 16

# Create base plot
plot(c(X1, X2), c(Y1, Y2), type = "n",
     xlab = "X-axis", ylab = "Y-axis",
     main = "Figure 2: Euclidean and Manhattan Distances",
     xlim = c(X1 - 4, X2 + 4), ylim = c(Y1 - 4, Y2 + 4))

# Plot points
points(X1, Y1, col = "red", pch = 16, cex = 2) 
points(X2, Y2, col = "blue", pch = 16, cex = 2)

# Add Manhattan path (green arrows)
arrows(X1, Y1, X2, Y1, col = "darkgreen", lwd = 2, length = 0.1)  # horizontal
arrows(X2, Y1, X2, Y2, col = "darkgreen", lwd = 2, length = 0.1)  # vertical

# Add Euclidean line (dashed purple)
segments(X1, Y1, X2, Y2, col = "purple", lwd = 2, lty = 2)

# Point labels
text(X1 - 0.5, Y1, labels = paste("(X1, Y1)\n(", X1, ",", Y1, ")"),
     col = "red", cex = 0.8, pos = 2)
text(X2 + 0.5, Y2, labels = paste("(X2, Y2)\n(", X2, ",", Y2, ")"),
     col = "blue", cex = 0.8, pos = 4)

# Euclidean distance label + arrow
text((X1 + X2)/2 - 1, (Y1 + Y2)/2 + 2.5, "Euclidean Distance (d)", col = "purple", font = 2, cex = 1)
arrows((X1 + X2)/2, (Y1 + Y2)/2 + 2, (X1 + X2)/2, (Y1 + Y2)/2 + 0.6,
       col = "purple", lwd = 1.5, length = 0.1)

# Manhattan label
text(X2 + 1.5, Y1 + 0.5, "Manhattan\nDistance", col = "darkgreen", font = 2, cex = 0.9, pos = 4)

# Euclidean distance formula
text(mean(c(X1, X2)), mean(c(Y1, Y2)) - 4.5, 
     labels = expression(d == sqrt((14 - 10)^2 + (16 - 12)^2)), 
     col = "black", cex = 0.9)

```
:::

::: notes
To find “closeness,” kNN uses distance metrics.
We tested two types: The most common is Euclidean distance, which measures straight-line distance, and Manhattan distance, which works like a city grid.
Since kNN relies on distance, the choice of metric — along with scaling — can significantly affect results.
This diagram shows both — we tested both in our project to compare results.
:::

## Classification Process {.center .middle}

::::: columns
::: {.column width="50%"}
<span class="fragment">The red square represents a data point to be
classified. The algorithm selects the 5 nearest neighbors within the
green circle—3 hearts and 2 circles. Based on the majority vote, the red
square is classified as a heart.
:::

::: {.column width="40%"}
![Figure 1. kNN with k=5](images/kNN_picture.png)
:::
:::::

::: notes
Here’s a simple example of how kNN makes a prediction.
The red square is a new point. It looks at the 5 nearest neighbors — in this case, 3 hearts and 2 circles.
Since hearts are the majority, the red square is predicted as a heart.
This simple majority voting process makes kNN easy to understand and explain - a big advantage in medical settings.
:::

## Strengths and Weaknesses of kNN {.smaller}

::::: columns
::: {.column width="50%"}
### Strengths

- **Simple, intuitive, and non-parametric** — no assumptions about data distribution  
- **No training phase** — the algorithm learns during prediction  
- **Performs well** on small to medium datasets, especially when features are well-scaled  
- **Easy to understand and implement** — ideal for baseline models or educational use
:::

::: {.column width="50%"}

### Weaknesses of kNN 

- **Slow prediction time** on large datasets due to distance calculations  
- **Sensitive to feature scaling and distance metric choice**  
- **Choosing the right 'k'** is critical — too low or high can reduce performance  
- **Affected by irrelevant or correlated features**, which may distort neighbor similarity

:::
:::::

::: notes
Here’s a quick summary of what makes kNN useful, and what challenges come with it.
It’s simple, doesn’t need training, and works well with smaller datasets when features are properly scaled. That’s why it’s often used for quick testing or in educational settings.
But it can struggle with large datasets or noisy features. It’s also sensitive to scaling, and choosing the right k value is really important.
That’s why preprocessing and tuning are so important - especially in healthcare, where accuracy and fairness matter.
:::

## Analysis and Results 

### Data Source and Collection {.smaller}

**Data Source:** CDC Diabetes Health Indicators

Collected via the CDC’s Behavioral Risk Factor Surveillance System
(BRFSS)

Dataset contains 253,680 survey responses

Covers 21 features: demographics, lifestyle, healthcare, and health
history

**Target:** Diabetes_binary

(0 = No diabetes, 1 = Diabetes/Prediabetes)

::: notes
For this project, we used real survey data from the CDC’s BRFSS program.
The dataset includes over 250,000 adult responses and covers a wide range of features like age, BMI, physical activity, and general health.
Our target was a binary variable indicating diabetes or prediabetes.
The dataset’s size and feature variety made it an ideal test case for evaluating how kNN behaves in a real-world health context.
:::

## Data Challenges {.smaller}

**Data Quality**  
[No missing values]{.fragment}  
[24,206 duplicate rows detected]{.fragment}

**Outliers & Scaling Sensitivity**  
[BMI, MentHlth, PhysHlth had extreme values]{.fragment}  
[kNN is highly sensitive to scale]{.fragment}

**Feature Relationships**  
[No multicollinearity (r < 0.5)]{.fragment}  
[All features retained for now]{.fragment}

**Early Insight**  
[Higher BMI in diabetic cases, but overlapping range]{.fragment}  
[Used as a predictor along with other features]{.fragment}

::: notes
Before modeling, we looked closely at the raw dataset.
There were no missing values, but nearly 10% of the data was duplicated - those could bias the model if not removed.
BMI and other health features showed outliers, and because kNN uses distance, that’s a problem - large values can dominate.
We also checked for correlation but didn’t find any features that were too closely related.
One early pattern we noticed: people with diabetes generally had higher BMI, but it wasn’t only factor.
:::

## Class Distribution: {.smaller}
### Diabetes Class Imbalance {.smaller}

::::: columns
::: {.column width="50%"}
**Key Points**\
[Significant class imbalance observed]{.fragment}\
[Majority class: No Diabetes (0) – **86.07%**]{.fragment}\
[Minority class: Diabetes (1) – **13.93%**]{.fragment}

**Impact on Modeling**\
[Imbalance can bias predictions]{.fragment}\
[Models may underpredict diabetes cases]{.fragment}
:::

::: {.column width="40%"}
```{r setup, include=FALSE}
library(reticulate)
use_python("C:/Users/Elena/miniconda3/envs/myenv/python.exe", required = TRUE)

```

```{python, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Real values
class_counts = pd.Series({0: 218334, 1: 35346})
class_percentages = class_counts / class_counts.sum() * 100

# Plot
plt.figure(figsize=(9, 6))
ax = sns.barplot(x=class_counts.index, y=class_counts.values, palette=["#66c2a5", "#fc8d62"])

# Smaller annotation text inside bars
for i, value in enumerate(class_counts.values):
    percentage = class_percentages[i]
    ax.text(i, value - 15000, f"{value:,}\n({percentage:.2f}%)", 
            ha="center", va="top", fontsize=10, fontweight='bold', color="white")

# Title and axes
plt.title("Figure 5: Class Distribution of Diabetes_binary", fontsize=16, fontweight='bold', pad=20)
plt.ylabel("Count")
plt.xlabel("Diabetes Status (0 = No, 1 = Diabetes/Prediabetes)")
plt.xticks([0, 1], ["No Diabetes", "Diabetes/Prediabetes"])
plt.ylim(0, max(class_counts.values) + 25000)

plt.tight_layout()
plt.show()

```
:::
:::::

::: notes
Another major challenge was class imbalance.
About 86% of cases were non-diabetic, and only 14% were diabetic or prediabetic.
This imbalance can cause models to favor the majority class and miss early diabetes cases.
We addressed this in preprocessing.
:::

## Preparing the Data {.smaller}

-   **Removed 24,206 duplicate rows**\
    [Diabetic class increased from 13.9% → 15.3%]{.fragment}

-   **Kept ordinal features as numeric**\
    [Age, Education, Income, and GenHlth retained due to natural ordering]{.fragment}

-   **Scaled Features with Outliers**\
    [BMI, MentHlth, PhysHlth scaled with StandardScaler & Robustscaler]{.fragment}

-   **Handled class imbalance**\
    [Applied SMOTE to generate synthetic diabetic samples]{.fragment}

➡️ Final dataset: **clean, scaled, and balanced**

::: notes
To get the data ready, we removed duplicates and kept ordinal features like age, education, and income as numeric.
The variables BMI, MentHlth, and PhysHlth were standardized using StandardScaler or RobustScaler to ensure equal contribution during distance calculations, a critical aspect for kNN’s accuracy.
To fix the class imbalance, we used SMOTE, which creates synthetic diabetic cases to help the model learn both classes equally.
After these steps, the dataset was clean, scaled, and balanced — ready for training our kNN models.
:::

## kNN Model Setup {.smaller}

-   Explored different **k values**: 5, 10, 15\
-   Compared **distance metrics**: Euclidean vs. Manhattan\
-   Evaluated **weighting methods**: uniform vs. distance\
-   Tested multiple **scaling techniques**\
-   Included variations with **SMOTE** and **Feature Selection**

::: {style="overflow-x: auto; font-size: 90%"}
| Model | k   | Distance        | Weights  | Scaler         | SMOTE                   |
|------------|------------|------------|------------|------------|------------|
| kNN 1 | 5   | Euclidean (p=2) | Uniform  | StandardScaler | No                      |
| kNN 2 | 15  | Manhattan (p=1) | Distance | RobustScaler   | No                      |
| kNN 3 | 10  | Euclidean (p=2) | Uniform  | StandardScaler | Yes                     |
| kNN 4 | 15  | Euclidean (p=2) | Distance | StandardScaler | Yes (Feature Selection) |
:::

::: notes
To better understand kNN’s behavior, we designed four model variations.
We changed the number of neighbors, distance type, and weighting method.
We also experimented with different scaling techniques and applied SMOTE to handle class imbalance.
In one version, we also used feature selection to reduce dimensional noise.
The final model - kNN 4 - combined all these enhancements and delivered the strongest performance overall.
:::

## Performance of kNN Variants {.smaller}
#### Table 3: Performance Comparison of kNN Models {.no-title}

::: {style="overflow-x: auto; font-size: 90%"}
| Model | k | Distance | Weights | Scaler | SMOTE | Accuracy | ROC_AUC | Precision_1 | Recall_1 | F1_1 |
|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| kNN 1 | 5 | Euclidean (p=2) | Uniform | StandardScaler | No | 0.83 | 0.70 | 0.41 | 0.21 | 0.27 |
| kNN 2 | 15 | Manhattan (p=1) | Distance | RobustScaler | No | 0.84 | 0.75 | 0.45 | 0.16 | 0.23 |
| kNN 3 | 10 | Euclidean (p=2) | Uniform | StandardScaler | Yes | 0.69 | 0.73 | 0.28 | 0.64 | 0.39 |
| kNN 4 | 15 | Euclidean (p=2) | Distance | StandardScaler | Yes (FS) | 0.78 | 0.88 | 0.73 | 0.88 | 0.80 |
:::

- **Best configuration: kNN 4**  
  [k = 15, Euclidean distance, distance weighting]{.fragment}  
  [StandardScaler, SMOTE, and feature selection]{.fragment}

- **Highest Weighted F1 Score: 0.80**  
  [Achieved recall = 0.88, precision = 0.73]{.fragment}

- 🩺 Most effective at identifying diabetic class (1)

::: notes
Table 3 shows the results for each kNN model.
As you can see, model performance varies significantly depending on configuration. For instance, KNN 1 and 2 without SMOTE had higher accuracy but poor recall - meaning they missed a lot of diabetic cases.
KNN 4, which combined SMOTE and feature selection, offered the best balance - best F1 score of 0.80 and a recall of 0.88 - especially for minority class detection.
That’s why we selected kNN 4 as the final model - it was the best at identifying diabetic patients fairly and consistently.
:::

## Comparing kNN with Tree Models {.smaller}
 
#### Table 4: Best kNN vs. Tree-Based Models {.no-title}

::: {style="overflow-x: auto; font-size: 90%"}
| Model         | SMOTE | Accuracy | ROC_AUC | Precision_1 | Recall_1 | F1_1 |
|---------------|-------|----------|---------|-------------|----------|------|
| KNN           | Yes   | 0.78     | 0.88    | 0.73        | 0.88     | 0.80 |
| Decision Tree | Yes   | 0.72     | 0.80    | 0.70        | 0.78     | 0.74 |
| Decision Tree | No    | 0.86     | 0.81    | 0.52        | 0.15     | 0.24 |
| Random Forest | No    | 0.87     | 0.82    | 0.59        | 0.13     | 0.21 |
:::


<span class="fragment">- **kNN achieved the highest F1 score** (0.80)
with strong recall on the diabetic class\

<span class="fragment">- **Decision Tree with SMOTE** performed
comparably but slightly lower on F1\

<span class="fragment">- **Random Forest had highest accuracy**, but
**poor recall** (0.13) shows it struggled to detect diabetic cases\

Tree-based models offer interpretability, but
may need tuning or resampling for minority detection

::: notes
We also compared the best kNN model to Decision Trees and Random Forests.
Random Forest had the highest accuracy — but very poor recall. It missed most diabetic cases.
The Decision Tree with SMOTE did better, but it still couldn’t match kNN’s balance of precision and recall.
Our tuned kNN outperformed both in detecting the minority class, making it a stronger choice for disease prediction.
:::

## ROC_AUC curves comparison {.smaller}

This plot compares the ROC curves for all four models.\
kNN with Feature Selection performs best (AUC = 0.88), followed by
Random Forest.

![ROC Curve](roc_curve.png)

::: notes
This ROC curve shows how well each model separates diabetic from non-diabetic cases.
Our best kNN model, with SMOTE and feature selection, had the highest AUC of 0.88, meaning it balanced true positives and false positives better than the others.
Random Forest and Decision Tree performed reasonably well, but neither matched kNN in class separation.
This supports the idea that a well-tuned kNN model can be both accurate and clinically effective.
:::

## Conclusion {.smaller}

- This project demonstrated that the **k-Nearest Neighbors (kNN)** algorithm can be an effective tool for disease prediction when properly tuned and supported by strong preprocessing.\

- Despite its simplicity, kNN achieved competitive results through careful configuration — including scaling, handling class imbalance, and feature selection.\

- Its interpretability, flexibility, and performance make it a practical choice in healthcare settings, where fairness and transparency are essential.\

- Ultimately, this work highlights how even basic algorithms, when thoughtfully applied, can deliver meaningful insights in real-world medical data.\

::: notes
In conclusion, our study shows that kNN is a strong candidate for disease prediction, especially when transparency and recall are priorities.
With thoughtful tuning, scaling, and SMOTE, kNN outperformed tree-based models in F1 score and minority class detection.
Despite being simple, it handled the diabetes prediction task very well - especially after reducing dimensional noise and balancing the data.
:::

## References {.smaller .scrollable}

Ali, AMEER, MOHAMMED Alrubei, LF Mohammed Hassan, M Al-Ja’afari, and Saif Abdulwahed. 2020. “Diabetes Classification Based on KNN.” IIUM Engineering Journal 21 (1): 175–81.

Altamimi, Abdulaziz, Aisha Ahmed Alarfaj, Muhammad Umer, Ebtisam Abdullah Alabdulqader, Shtwai Alsubai, Tai-hoon Kim, and Imran Ashraf. 2024. “An Automated Approach to Predict Diabetic Patients Using KNN Imputation and Effective Data Mining Techniques.” BMC Medical Research Methodology 24 (1): 221.

Boateng, Emmanuel B. 2020. “Basic Concept of Classification Algorithms for Machine Learning.” ResearchGate. https://doi.org/10.13140/RG.2.2.23226.08648.

Deng, Zhenyun, Xiaoshu Zhu, Debo Cheng, Ming Zong, and Shichao Zhang. 2016. “Efficient kNN Classification Algorithm for Big Data.” Neurocomputing 195: 143–48.

Iparraguirre-Villanueva, Orlando, Karina Espinola-Linares, Rosalynn Ornella Flores Castañeda, and Michael Cabanillas-Carbonell. 2023. “Application of Machine Learning Models for Early Detection and Accurate Classification of Type 2 Diabetes.” Diagnostics 13 (14): 2383.

Kataria, Aman, and MD Singh. 2013. “A Review of Data Classification Using k-Nearest Neighbour Algorithm.” International Journal of Emerging Technology and Advanced Engineering 3 (6): 354–60.

Khateeb, Nida, and Muhammad Usman. 2017. “Efficient Heart Disease Prediction System Using k-Nearest Neighbor Classification Technique.” In Proceedings of the International Conference on Big Data and Internet of Thing, 21–26.

Mucherino, Antonio, Petraq J Papajorgji, Panos M Pardalos, Antonio Mucherino, Petraq J Papajorgji, and Panos M Pardalos. 2009. “K-Nearest Neighbor Classification.” Data Mining in Agriculture, 83–106.

Panwar, Madhuri, Amit Acharyya, Rishad A Shafik, and Dwaipayan Biswas. 2016. “K-Nearest Neighbor Based Methodology for Accurate Diagnosis of Diabetes Mellitus.” In 2016 Sixth International Symposium on Embedded Computing and System Design (ISED), 132–36. IEEE.

Saxena, Krati, Zubair Khan, and Shefali Singh. 2014. “Diagnosis of Diabetes Mellitus Using k Nearest Neighbor Algorithm.” International Journal of Computer Science Trends and Technology (IJCST) 2 (4): 36–43.

Suriya, S, and J Joanish Muthu. 2023. “Type 2 Diabetes Prediction Using k-Nearest Neighbor Algorithm.” Journal of Trends in Computer Science and Smart Technology 5 (2): 190–205.

Syriopoulos, Panos K, Nektarios G Kalampalikis, Sotiris B Kotsiantis, and Michael N Vrahatis. 2023. “K NN Classification: A Review.” Annals of Mathematics and Artificial Intelligence, 1–33.

Theerthagiri, Prasannavenkatesan, A Usha Ruby, and J Vidya. 2022. “Diagnosis and Classification of the Diabetes Using Machine Learning Algorithms.” SN Computer Science 4 (1): 72.

Uddin, Shahadat, Ibtisham Haque, Haohui Lu, Mohammad Ali Moni, and Ergun Gide. 2022. “Comparative Performance Analysis of k-Nearest Neighbour (KNN) Algorithm and Its Different Variants for Disease Prediction.” Scientific Reports 12 (1): 6256.

Zhang, Shichao, Xuelong Li, Ming Zong, Xiaofeng Zhu, and Ruili Wang. 2017. “Efficient kNN Classification with Different Numbers of Nearest Neighbors.” IEEE Transactions on Neural Networks and Learning Systems 29 (5): 1774–85.

Zhang, Zhongheng. 2016. “Introduction to Machine Learning: K-Nearest Neighbors.” Annals of Translational Medicine 4 (11).


