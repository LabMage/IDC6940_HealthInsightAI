---
title: "Writing a great story for data science projects - spring 2025"
subtitle: "This is a Report Template Quarto"

author: "Boiko Elena & Razo Jaquelin (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
  default-language: python
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!
:::


## Introduction: 
KNN is an algorithm that is being used in a variety of fields to classify or predict data. The KNN algorithm is a simple algorithm that classifies data based on how similar a datapoint is to a class of datapoints. One of the benefits of using this algorithmic model is how simple it is to use and the fact it’s non-parametric which means it fits a wide variety of datasets. One drawback from using this model is that it does require a higher computational need than other models which means that it doesn’t perform as well on big data. Despite this, the model’s simplicity makes it easy to understand and easy to implement in a variety of fields. One such field is the field of healthcare where KNN models have been successfully used to predict diseases such as diabetes and hypertension. In this paper we will focus on its methodology and application in the field of healthcare to predict diabetes, a pressing public health problem. 

## Literature Review: 

## Overview of machine learning
Machine learning is a promising field where computers are able to learn from prior data to perform a function such as regression, classification or even create new values or images. KNNs have emerged as easy to understand algorithms that are able to perform regression, classification, data imputation and many other uses. 

## Theoretical background of KNN

KNNs are supervised learning algorithms that work by comparing a data point to other similar data points in order to label it. In the thesis (Zhang), the author gave the reader an introduction into how KNN works and how to run it in R studio. He describes the methodology as assigning an unlabeled observation to a class by using labeled examples that are similar to it. It also describes the euclidean distance equation which is the default distance equation that is used for KNNs. The author offered alternatives to it like the Manhattan distance. The author also describes the impact the k has on the algorithm. It recommends setting the k equal to the square root of the number of observations in the training dataset. 
In the thesis ( A Review of Data Classification Using K-Nearest Neighbour Algorithm) This paper focused on comparing different distances in classification algorithms with a focus on the KNN algorithm. It explains the methodology used in KNN. It states the KNN algorithm uses the nearest k-neighbors in order to classify data points. The paper states KNN uses the euclidean distance by default and it compares its performance using cityblock, cosine and correlation distances. In the end it found the euclidean distance was more efficient than the others. 

Overall, the paper (kNN Classification: A Review)presents a well-structured and informative review of kNN classification. It provides an extensive theoretical background, explores key variations, and discusses computational efficiency strategies. Since this work is primarily a literature review, it compiles research findings without introducing new experimental analysis. The inclusion of software resources makes it a practical guide for researchers and practitioners. This paper serves as a foundational resource for anyone interested in kNN-based classification and its evolving applications [@syriopoulos2023k] 

## Performance evaluations and challenges

## Improvements on KNN
In the thesis (Efficient kNN Classification With Different Numbers of Nearest Neighbors) the author proposes two more efficient KNN methods called the kTree and the k*Tree methods. The authors propose using a training stage using a decision tree to select the ideal number of K values to make the KNN more efficient. The authors found their method did reduce running cost and increased classification accuracy.
The goal of Efficient kNN classification algorithm for big data is to be able to use KNN in big data. This paper proposes using a k-means clustering technique to separate big data dataset into several parts to make the KNN more efficient. The reason this paper is important is because KNN traditionally work by measuring the euclidean distance between different datasets to see what the nearest k neighbors are and this can be computationally expensive with big datasets. The authors proposal would lead to a more efficient method of using knn. The authors concluded the LC-KNN worked the best because it had the highest accuracy and best time [@deng2016efficient] 

## Application of KNN in different fields

	The thesis (DIABETES CLASSIFICATION BASED ON KNN) discusses the use of different types of KNN in order to classify diabetes. It does this by using matlab and six different types of KNN algorithms to classify glucose concentrations in the blood. The paper starts of with an explanation of how KNN algorithms work by using the Euclidean distance between the different classes. The KNN algorithm takes the closes N number of neighbors and assigns the class based on the nearest neighbors. Out of the six algorithms that were used the fine KNN was the one that was the most successful at correctly classifying the data. [@ali2020diabetes]
	
	The goal of this paper (Diagnosis of Diabetes Mellitus using K Nearest Neighbor Algorithm).The goal of this paper is to diagnose diabetes mellitus using KNN. This paper describes what diabetes mellitus is and the health problems it causes in individuals. This paper is important because it can lead doctors to use better detection tools to detect diabetes mellitus. This paper describes the way the KNN algorithm works in order to detect diabetes mellitus. The algorithm is described as taking a sample dataset with input variables and one output variable, taking a test dataset from the sample, finding the distance between each dataset in the training sample with the euclidean formula, then deciding the random variable of k where k is the number of nearest neighbors you will use to classify. Then it does the same thing to the test dataset and uses the K nearest members to find the output variable. After the classification, the authors go into detail of how they analyzed the results based on specificity. Finally, the paper describes the way they used mathlab to check for the accuracy of their results. The accuracy for a K of 3 was 70% and the accuracy for a K of 5 was 75%. 
	
Several studies have explored the use of KNN for diabetes predictions, often using publicly available datasets such as the Pima Indian Diabetes Dataset (PIDD). For example:

The study of (Type 2 Diabetes Prediction using K-Nearest Neighbor Algorithm) showed that KNN is a promising model for predicting type 2 diabetes, showing the highest accuracy on smaller datasets. The authors tested three datasets of varying sizes from 692 to 1853 rows and 9-22 dimensions to test the KNN algorithm's performance and found that the larger dataset requires a higher k-value. Besides, PCA analysis to reduce dimensionality did not improve model performance. This suggests that simplifying the data doesn’t always lead to better results in diabetes prediction [@suriya2023type]

The same findings about PCA influence on ML models implementation, and KNN in particular, showed in the research (Application of Machine Learning Models for Early Detection and Accurate Classification of Type 2 Diabetes). The authors compare the effectiveness of machine learning (ML) models, including K-Nearest Neighbors (K-NN), Bernoulli Naïve Bayes (BNB), Decision Tree (DT), Logistic Regression (LR), and Support Vector Machine (SVM), for early detection and classification of type 2 diabetes. While PCA significantly reduced accuracy for all models, the SMOTE-preprocessed dataset demonstrated the highest accuracy for the K-NN model (79.6%), followed by BNB with 77.2%. This reveals the importance of correct preprocessing techniques in improving KNN model accuracy, especially when handling imbalanced datasets. [@iparraguirre2023application]

Panwar et al. (K-nearest neighbor-based methodology for accurate diagnosis of diabetes mellitus) also demonstrated the critical role of data preprocessing and dimensionality reduction in improving the accuracy of KNN models for diabetes diagnosis. Their study found that by focusing on just two key features (BMI and Diabetes Pedigree Function) the model achieved its highest accuracy, sensitivity, and specificity after careful preprocessing. Comparative analyses showed that the proposed methodology outperformed other approaches, such as multi-layer neural networks (MLNNs) and Support Vector Machines (SVMs). Additionally, cross-validation confirmed the model's reliability, emphasizing the importance of proper data handling in the KNN model for diabetes classification [panwar2016k]


Diagnosis of Diabetes Mellitus using K Nearest Neighbor Algorithm

While standard KNN is effective, its performance varies depending on distance metrics, feature weighting, and hyperparameter selection. Uddin et al. (Comparative Performance Analysis of K-Nearest Neighbour (KNN) Algorithm and its Different Variants for Disease Prediction) explored KNN variants across multiple disease prediction tasks, including:
Weighted KNN - assigns greater influence on closer neighbors.
Distance-Weighted KNN - uses Manhattan, Euclidean, and Hassanat distances for improved classification.
Ensemble Approach KNN - achieved the highest precision and adaptability, outperforming standard KNN in disease prediction.
Despite its improvements in precision and recall, the computational cost of KNN variants remains a challenge, particularly in large-scale medical applications. Also, performance heavily depended on the choice of k and the distance metric. 
Facing these challenges, the authors suggested exploring assembled models that combine KNN with other machine-learning techniques for better efficiency and accuracy. [@uddin2022comparative]

The effectiveness of KNN extends beyond diabetes classification, as shown in the study by Khateeb & Usman (Efficient Heart Disease Prediction System Using K-Nearest Neighbor Classification Technique ) on heart disease prediction. Their work demonstrates that feature selection and data balancing techniques significantly impact KNN’s classification performance. The authors evaluated different feature selection strategies, showing that removing irrelevant attributes (e.g., age, sex, fasting blood sugar) sometimes reduced accuracy rather than improving it. Research also reveals the importance of data resampling in improving accuracy. [@khateeb2017efficient]


*This is my work and I want to add more work...*

## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*


*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References

```{python}

```


```{python}
# Python code here
print("This is a Python chunk in Quarto!")
```
```{r}

```


