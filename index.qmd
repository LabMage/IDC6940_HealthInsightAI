---
title: "Diagnosing diseases using KNN- Spring 2025"
subtitle: "An Application of KNN to Diagnose Diabetes"
author: "Jacqueline Razo & Elena Boiko (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!
:::


# Introduction
The k-Nearest-Neighbors (kNN) is an algorithm that is being used in a variety of fields to classify or predict data. The kNN algorithm is a simple algorithm that classifies data based on how similar a datapoint is to a class of datapoints. One of the benefits of using this algorithmic model is how simple it is to use and the fact it’s non-parametric which means it fits a wide variety of datasets. One drawback from using this model is that it does have a higher computational cost than other models which means that it doesn’t perform as well or fast on big data. Despite this, the model’s simplicity makes it easy to understand and easy to implement in a variety of fields. One such field is the field of healthcare where kNN models have been successfully used to predict diseases such as diabetes and hypertension. In this paper we will focus on the methodology and application of kNN models in the field of healthcare to predict diabetes, a pressing public health problem. 
 This literature review explores: 
 
1.	The theoretical background of KNN and key factors affecting its performance.

2.	Recent advancements in optimizing KNN for large datasets.

3.	The role of KNN in medical diagnosis, particularly diabetes prediction.

# Literature Review 
## Theoretical Background of KNN
KNNs are supervised learning algorithms that work by comparing a data point to other similar data points to label it. It works on the assumption that data points that are similar to each other must be close to each other. In the thesis [@zhang2016introduction], the author gave the reader an introduction to how KNN works and how to run a KNN model in R studio. He describes the methodology as assigning an unlabeled observation to a class by using labeled examples that are similar to it. It also describes the Euclidean distance equation which is the default distance equation that is used for KNNs. The author also describes the impact the k parameter has on the algorithm. The K parameter is the parameter that tells the model how many neighbors it will use when trying to classify a data point. Zhang recommends setting the k parameter equal to the square root of the number of observations in the training dataset.

However, tuning K values dynamically can improve model efficiency. Although Zhang’s recommendation to set the k parameter could be a great starting point, the thesis [@zhang2017efficient] proposed the decision tree-assisted tuning to optimize K, significantly enhancing accuracy.  The authors of this thesis propose using a training stage where we use a decision tree to select the ideal number of K values and thus make the KNN more efficient. The authors deployed and tested two more efficient KNN methods called kTree and the k*Tree methods. They found their method did reduce running costs and increased classification accuracy.
    
Another big impact on accuracy is the distance the model uses to classify neighbors. Although the euclidean distance is the default distance that is used in kNNs there are other distances that can be used. In the thesis [@kataria2013review] the authors compare different distances in classification algorithms with a focus on the kNN algorithm. It starts off explaining how the KNN algorithm uses the nearest k-neighbors in order to classify data points and then describes how the euclidean distance does this by putting a line segment between point a and point b and then measuring the distance using the euclidean distance formula. It moves on to describe the “cityblock” or taxican distance and describes it as “the sum of the length of the projections of the line segment”. It also describes the cosine distance and the correlation distance and then compares the performance of the default euclidean distance to the performance of using city block, cosine and correlation distances. In the end it found the euclidean distance was more efficient than the others in their observations.
    
Syriopoulos et al. [@syriopoulos2023k] also reviewed distance metric selection, confirming that Euclidean distance remains the most effective choice for most datasets. However, alternative metrics like Mahalanobis distance can perform better for correlated features. The review emphasized that selecting the right metric is dataset-dependent, influencing classification accuracy.
    
## Challenges in Scaling KNN for Large Datasets
While KNN is simple and effective, it struggles with computational inefficiency when working with large datasets since it must calculate distances for every new observation. This becomes a major challenge in big data, where the sheer volume of information makes traditional KNN methods slow and resource-intensive.
    
To address this, Deng et al. [@deng2016efficient] proposed an improved approach called LC-KNN, which combines K-means clustering with KNN to speed up computations and enhance accuracy. By dividing large datasets into smaller clusters, their method reduces the number of distance calculations needed. After extensive testing, the authors found that LC-KNN consistently outperformed standard KNN, achieving higher accuracy and better efficiency. Their study highlights a key limitation of traditional KNN (without optimization, its performance significantly declines on big data) and offers an effective solution to improve its scalability.
     
Continuing and summarizing these ideas, Syriopoulos et al. [@syriopoulos2023k] explored techniques for accelerating KNN computations, such as:
    
•	Dimensionality reduction (e.g., PCA, feature selection) to reduce data complexity.

•	Approximate Nearest Neighbor (ANN) methods to speed up distance calculations.

•	Hybrid models combining KNN with clustering (e.g., LC-KNN) to improve efficiency.

This approach enhanced both speed and accuracy, making it a promising solution for handling large datasets. In addition, the study categorizes KNN modifications into local hyperplane methods, fuzzy-based models, weighting schemes, and hybrid approaches, demonstrating how these adaptations help tackle issues like class imbalance, computational inefficiency, and sensitivity to noise.

Another key challenge for KNN is its performance in high-dimensional datasets. The 2023 study by Syriopoulos et al. evaluates multiple nearest neighbor search algorithms such as kd-trees, ball trees, Locality-Sensitive Hashing (LSH), and graph-based search methods that enable KNN performance scaling for larger datasets through minimized distance calculations.

The enhancements to KNN have substantially increased its performance in terms of speed and accuracy which now allows it to better handle large-scale datasets. However, as Syriopoulos et al. primarily compile prior research rather than conducting empirical comparisons, further work is needed to evaluate these optimizations in real-world medical classification tasks.

## KNN in Disease Prediction: Applications & Limitations
### Disease Prediction with KNN
KNN has been widely used for diabetes classification and early detection. Ali et al. [@ali2020diabetes] tested six different KNN variants in MATLAB to classify blood glucose levels, finding that fine KNN was the most accurate. Their research highlights how optimizing KNN can improve classification performance, making it a valuable tool in healthcare.

In turn, Saxena et al. [@saxena2014diagnosis] used KNN on a diabetes dataset and observed that increasing the number of neighbors (K) led to better accuracy, but only to a certain extent. In their MATLAB-based study, they found that using K = 3 resulted in 70% accuracy, while increasing K to 5 improved it to 75%.
Both studies demonstrate how KNN can effectively classify diabetes, with accuracy depending on the choice of K and dataset characteristics. Ongoing research continues to refine KNN, making it a more efficient and reliable tool for medical applications.

Feature selection is another critical factor. Panwar et al. [@panwar2016k] demonstrated that focusing on just BMI and Diabetes Pedigree Function improved accuracy, suggesting that simplifying feature selection enhances model performance.

The study of Suriya and Muthu [@suriya2023type] showed that KNN is a promising model for predicting type 2 diabetes, showing the highest accuracy on smaller datasets. The authors tested three datasets of varying sizes from 692 to 1853 rows and 9-22 dimensions to test the KNN algorithm’s performance and found that the larger dataset requires a higher k-value. Besides, PCA analysis to reduce dimensionality did not improve model performance. This suggests that simplifying the data doesn’t always lead to better results in diabetes prediction.

The same findings about PCA influence on ML models implementation, and KNN in particular, showed in the research of Iparraguirre-Villanueva et al. [@iparraguirre2023application].  Also, they confirmed that KNN alone is not always the best choice. Authors compared KNN with Logistic Regression, Naïve Bayes, and Decision Trees. Their results showed that while KNN performed well on balanced datasets, it struggled when class imbalances existed. While PCA significantly reduced accuracy for all models, the SMOTE-preprocessed dataset demonstrated the highest accuracy for the K-NN model (79.6%), followed by BNB with 77.2%. This reveals the importance of correct preprocessing techniques in improving KNN model accuracy, especially when handling imbalanced datasets.

Khateeb & Usman [@khateeb2017efficient] extended KNN’s application to heart disease prediction, demonstrating that feature selection and data balancing techniques significantly impact accuracy. Their study showed that removing irrelevant features did not always improve performance, emphasizing the need for careful feature engineering in medical datasets.

## KNN Beyond Prediction: Handling Missing Data
While KNN is widely known for classification, it also plays a key role in data preprocessing for medical machine learning. Altamimi et al. [@altamimi2024automated]  explored KNN imputation as a method to handle missing values in medical datasets. Their study showed that applying KNN imputation before training a machine learning model significantly improved diabetes prediction accuracy - from 81.13% to 98.59%. This suggests that KNN is not only useful for disease classification but also for improving data quality and completeness in healthcare applications.

Traditional methods often discard incomplete records, but KNN imputation preserves valuable information, leading to more reliable model performance. However, Altamimi et al. (2024) also highlighted challenges such as computational costs and sensitivity to parameter selection, reinforcing the need for further optimization when applying KNN to large-scale medical datasets.

## Comparing KNN Variants & Hybrid Approaches
Research indicate that KNN works well for diabetes prediction, but recent studies demonstrate it doesn't consistently provide the best results. The study by Theerthagiri et al.  [@theerthagiri2022diagnosis] evaluated KNN against multiple machine learning models such as Naïve Bayes, Decision Trees, Extra Trees, Radial Basis Function (RBF), and Multi-Layer Perceptron (MLP) through analysis of the Pima Indians Diabetes dataset. The research indicated that KNN performed adequately but MLP excelled beyond all other algorithms achieving top accuracy at 80.68% and leading in AUC-ROC with an 86%. Despite its effectiveness in classification tasks, KNN's primary limitation is its inability to compete with advanced models like neural networks when processing complex datasets.

In turn, Uddin et al.[@uddin2022comparative] explored advanced KNN variants, including Weighted KNN, Distance-Weighted KNN, and Ensemble KNN. Their findings suggest that:

•	Weighted KNN improved classification by assigning greater importance to closer neighbors.

•	Ensemble KNN outperformed standard KNN in disease prediction but required additional computational resources.

•	Performance was highly sensitive to the choice of distance metric and K value tuning.

Their findings suggest that KNN can be improved through modifications, but it remains highly sensitive to dataset size, feature selection, and distance metric choices. In large-scale healthcare applications, Decision Trees (DT) and ensemble models may offer better trade-offs between accuracy and efficiency.
These studies highlight the ongoing debate over KNN’s role in medical classification - whether modifying KNN is the best approach or if other models, such as DT or ensemble learning, provide stronger performance for diagnosing diseases.

## Summary
KNN continues to be a valuable tool in medical machine learning, offering simplicity and strong performance in classification tasks. However, as research shows, its effectiveness depends on proper feature selection, optimized K values, and preprocessing techniques like imputation. While KNN remains an interpretable and adaptable model, newer methods - such as ensemble learning and neural networks - often outperform it, particularly in large-scale datasets.
For our capstone project, exploring feature selection, fine-tuning KNN’s settings, and comparing it to other algorithms could give us valuable insights into its strengths and limitations.


# Methods

The k-Nearest Neighbors (kNN) algorithm is a non-parametric, distance-based machine learning approach that classifies data points based on their similarity to labeled examples. It operates under the assumption that similar instances exist in close proximity within the feature space. The classification process follows three main steps:
1.	## Distance Calculation:
The algorithm measures the similarity between a new data point and all training instances using a distance metric. The most common choice is Euclidean distance, given by the formula:


$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.

*Where $X_2 - X_1$ calculates the horizontal difference and $Y_2 - Y_1$ calculates the vertical difference. These two distances are then squared to ensure they are positive regardless of which directionality. Squaring the distances also gives greater emphasis to larger distances.* 
Other distance metrics, such as Manhattan distance, Minkowski distance, or cosine similarity, may also be used depending on the dataset and problem requirements [@kataria2013review].
2.	## Neighbor Selection (k-value):
The algorithm identifies the k closest data points to the new observation. The choice of k is crucial—
A small k (e.g., k=1 or 3) increases sensitivity to noise, making the model prone to overfitting.
A large k (e.g., k=20 or more) smooths the decision boundary but may lead to underfitting [@zhang2017efficient].
Studies recommend using cross-validation or heuristic methods, such as setting k to the square root of the dataset size, to determine an optimal value [@syriopoulos2023k].
3.	## Majority Voting (Classification Decision):
Once the k-nearest neighbors are identified, the algorithm assigns the new data point the most frequent class label among its neighbors. In cases of ties, distance-weighted voting can be applied, where closer neighbors have higher influence on the classification decision [@uddin2022comparative].
## Enhancements for kNN Performance in Medical Datasets
While kNN is widely used for disease classification, including diabetes prediction, it has some limitations, particularly regarding computation time and sensitivity to high-dimensional data. Recent research suggests the following optimizations:
### Feature Scaling:
Since kNN relies on distance calculations, variables with larger ranges dominate those with smaller ones. MinMax Scaling or Standardization ensures that all features contribute equally to distance computations [@syriopoulos2023k].
### Feature Selection & Dimensionality Reduction:
High-dimensional datasets can lead to poor performance due to the curse of dimensionality. Methods such as Principal Component Analysis (PCA) and LASSO regression help retain the most relevant features while reducing noise in the data [@iparraguirre2023application].
### Handling Class Imbalance:
Imbalanced datasets, where one class significantly outnumbers the other (e.g., non-diabetic vs. diabetic cases), can bias kNN predictions. Techniques such as Synthetic Minority Over-sampling (SMOTE) can generate synthetic samples to balance class distributions [@suriya2023type].
### Optimized Search Techniques:
To improve efficiency for large datasets, kd-trees and Locality-Sensitive Hashing (LSH) help speed up nearest neighbor searches, making kNN feasible for big data applications [@deng2016efficient].




*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*


*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

We explored the CDC Diabetes Health Indicators dataset which was sourced from the UC Irvine Machine Learning Repository. This dataset was funded by the CDC in order to get insights into lifestyle factors and their relationship to diabetes. The data was imported using python code to install the ucimlrepo package. We used the import instructions listed on the UCI website. 

The dataset includes 253,680 entries with 21 features and target variable, covering health indicators like high blood pressure, cholesterol levels, BMI, smoking habits, physical activity, and socioeconomic factors.

### Loading the dataset

To import the dataset, we used Python and the ucimlrepo package following the import instructions provided on the UCI website.

Python Environment Details:

Python Version: 3.9.16 (Conda)

Architecture: 64-bit

Key Libraries: numpy, pandas, matplotlib, seaborn

The dataset was successfully loaded into a Pandas DataFrame and displayed to confirm its structure and correctness.

```{r}
# Load required R packages
library(reticulate)  # Required for Python-R integration
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)

# Set Python environment (update this with your path)
use_python("C:\\Users\\Elena\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\python.exe", required = TRUE)

# Confirm Python setup
py_config()

```


```{r}
# Install Python package if not available
if (!py_module_available("ucimlrepo")) {
  py_install("ucimlrepo")
}

# Load Python and fetch dataset
py_run_string("
import pandas as pd
from ucimlrepo import fetch_ucirepo

# Fetch dataset
cdc_diabetes_health_indicators = fetch_ucirepo(id=891)

# Convert to Pandas DataFrame
df = pd.concat([cdc_diabetes_health_indicators.data.features, 
                cdc_diabetes_health_indicators.data.targets], axis=1)

# Save for R usage
df.to_csv('diabetes_dataset.csv', index=False)
")

```

### Displaying the first few rows.

```{r}
# Read the processed dataset into R
df <- read.csv("diabetes_dataset.csv")

# Show first rows
head(df)

```


```{python}

# variable information 
print(cdc_diabetes_health_indicators.variables)

```


## Exploratory Data Analysis (EDA)


```{python}

# Display feature names
print(cdc_diabetes_health_indicators.data.features.columns)

# Display target (y) variable name
print(cdc_diabetes_health_indicators.data.targets.columns)

```
```{python}
import pandas as pd

# Convert dataset to Pandas DataFrame
df = pd.concat([cdc_diabetes_health_indicators.data.features, 
                cdc_diabetes_health_indicators.data.targets], axis=1)

# Show basic information
df.info()  # Check column names & data types

```




```{python}

# Display basic information about the dataset
df_info = df.info()
df_info

```


```{python}

# Check for missing values
missing_values = df.isnull().sum()

missing_values

```
### Dataset Summary:

Rows: 253,680
Columns: 21 (all numerical, mostly categorical or ordinal)
No missing values: All columns have full data.

#### Feature types:

Binary: HighBP, HighChol, CholCheck, Smoker, Stroke, HeartDiseaseorAttack, PhysActivity, Fruits, Veggies, HvyAlcoholConsump, AnyHealthcare, NoDocbcCost, DiffWalk, Sex

Ordinal: GenHlth (1-5), Age (coded in categories 1-13), Education (1-6), Income (1-8)
Continuous: BMI, MentHlth, PhysHlth



```{python}

# Get basic statistics
df_stats = df.describe()
df_stats

```
#### Key Findings from EDA

Class Imbalance: Only 13.9% of people have diabetes, which suggests an imbalance in the target variable. This may require oversampling (SMOTE) or class weighting when training models.
BMI and High Blood Pressure are Major Health Concerns:
•	The average BMI is 28.38, close to the overweight range.
•	43% of the population has high blood pressure, which is a known risk factor for diabetes.
Physical Activity and Diet Indicators:
•	75% of individuals engage in regular physical activity.
•	81% eat vegetables regularly, and 63% eat fruits regularly, suggesting generally healthy dietary habits.
Age and Income Influence Health Outcomes:
•	Older individuals are more likely to develop diabetes.
•	Higher income groups tend to report better health, which may correlate with healthcare access.


## Visualizations for Exploration

```{r}
library(reticulate)

# Install missing Python packages
py_install("matplotlib")
py_install("seaborn")

```


```{python}

import sys
import matplotlib
import seaborn

print(f"Python version: {sys.version}")
print(f"Matplotlib version: {matplotlib.__version__}")
print(f"Seaborn version: {seaborn.__version__}")

```
```{r}
library(reticulate)

# Install all required Python packages with correct names
py_install(c("matplotlib", "seaborn", "pandas", "numpy", "scipy", "scikit-learn", "ipython"))


```

```{r}
system('conda config --add channels defaults')

```


```{python}

import matplotlib.pyplot as plt
import seaborn as sns

# Ensure the target variable is correctly defined
target_variable = "Diabetes_binary" if "Diabetes_binary" in df.columns else None

## Plot BMI Distribution**
plt.figure(figsize=(8, 5))
sns.histplot(df["BMI"], bins=50, kde=True, color="blue")
plt.title("BMI Distribution")
plt.xlabel("BMI")
plt.ylabel("Count")
plt.show()


```


```{python}

## Correlation Heatmap (Including Target Variable)**
plt.figure(figsize=(12, 8))

# Compute correlation only for numeric columns
corr_matrix = df.corr(numeric_only=True)

# Plot heatmap
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()


```




```{python}

##  Class Distribution of Target Variable (Diabetes_binary)**
if target_variable:
    plt.figure(figsize=(6, 4))
    sns.countplot(x=df[target_variable], palette="Set2")
    plt.title(f"Class Distribution of {target_variable}")
    plt.xlabel("Diabetes Status (0 = No, 1 = Diabetes/Prediabetes)")
    plt.ylabel("Count")
    plt.show()
else:
    print("No target variable detected. Please confirm which column represents diabetes.")

```



## "BMI Distribution by Diabetes Status"

```{python}

## Boxplot of BMI by Diabetes Status**
if target_variable:
    plt.figure(figsize=(8, 5))
    sns.boxplot(x=df[target_variable], y=df["BMI"], palette="Set3")
    plt.title("BMI Distribution by Diabetes Status")
    plt.xlabel("Diabetes Status (0 = No, 1 = Diabetes/Prediabetes)")
    plt.ylabel("BMI")
    plt.show()

```


```{python}





```
```{r}


```



```{python}


```




-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}

```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

-   Summarize your key findings.

-   Discuss the implications of your results.

## References



```

```{python}
# variable information 
print(cdc_diabetes_health_indicators.variables) 
```



```{python}
# Python code here
print("This is a Python chunk in Quarto!")
```



